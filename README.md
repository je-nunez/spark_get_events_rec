#

Get stages and tasks performance in Apache Spark (tested with Apache Spark 2.3.1)

The underlying example code to demo with performance data was taken from: [https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala)

# WIP

This project is a *work in progress*. The implementation is *incomplete* and
subject to change. The documentation can be inaccurate.

# Example Output

The sequence of `SparkListenerEvent` events are printed, with a maximum
recursion depth of up to 3 levels inside the case-class. (It is not a JSON
output of each event in order to be properly stored into a JSON log analyzer:
it is just some string representation of the case-classes extending
`SparkListenerEvent` in order for a human to read it, and possibly to search
on this results.)

Some results in an example:
          
          1535599573271: org.apache.spark.scheduler.SparkListenerJobStart(stageIds -> ArrayBuffer(0), jobId -> 0, properties: java.util.Properties -> java.util.Properties(serialVersionUID -> 4112578634029874840, defaults -> null, hexDigit: [C -> [C()), time -> 1535599573269, stageInfos -> WrappedArray(org.apache.spark.scheduler.StageInfo@209cb25e))
          1535599573291: org.apache.spark.scheduler.SparkListenerStageSubmitted(stageInfo: org.apache.spark.scheduler.StageInfo -> org.apache.spark.scheduler.StageInfo(name -> treeAggregate at LogisticRegression.scala:518, accumulables: scala.collection.mutable.HashMap -> scala.collection.mutable.HashMap(serialVersionUID -> 1, _loadFactor -> 750, seedvalue -> 4, table: [Lscala.collection.mutable.HashEntry; -> [Lscala.collection.mutable.HashEntry;(), sizemap -> null, threshold -> 12, tableSize -> 7), submissionTime: scala.Option -> scala.Some(serialVersionUID -> 1234815782226070388, x -> 1535599573291), stageId -> 0, parentIds -> List(), attemptId -> 0, completionTime: scala.Option -> scala.Some(serialVersionUID -> 1234815782226070388, x -> 1535599573599), taskLocalityPreferences -> Stream(List(), ?), numTasks -> 4, details -> org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1131) ... call-stack-details-omitted-in-this-sample-extract ... sbt.Run.invokeMain(Run.scala:93), failureReason: scala.Option -> scala.None$(MODULE$: scala.None$ -> scala.None$(MODULE$ -> None, serialVersionUID -> 5066590221178148012), serialVersionUID -> 5066590221178148012), taskMetrics: org.apache.spark.executor.TaskMetrics -> org.apache.spark.executor.TaskMetrics(tempShuffleReadMetrics -> null, bitmap$trans$0 -> 2, testAccum: scala.Option -> scala.None$(MODULE$ -> None, serialVersionUID -> 5066590221178148012), _peakExecutionMemory: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _executorDeserializeCpuTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 42236477, _count -> 0), nameToAccums -> Map(internal.metrics.executorDeserializeTime -> LongAccumulator(id: 1, name: Some(internal.metrics.executorDeserializeTime), value: 95), internal.metrics.executorDeserializeCpuTime -> LongAccumulator(id: 2, name: Some(internal.metrics.executorDeserializeCpuTime), value: 42236477), internal.metrics.executorRunTime -> LongAccumulator(id: 3, name: Some(internal.metrics.executorRunTime), value: 480), internal.metrics.executorCpuTime -> LongAccumulator(id: 4, name: Some(internal.metrics.executorCpuTime), value: 132684773), internal.metrics.resultSize -> LongAccumulator(id: 5, name: Some(internal.metrics.resultSize), value: 264790), internal.metrics.jvmGCTime -> LongAccumulator(id: 6, name: Some(internal.metrics.jvmGCTime), value: 0), internal.metrics.resultSerializationTime -> LongAccumulator(id: 7, name: Some(internal.metrics.resultSerializationTime), value: 5), internal.metrics.memoryBytesSpilled -> LongAccumulator(id: 8, name: Some(internal.metrics.memoryBytesSpilled), value: 0), internal.metrics.diskBytesSpilled -> LongAccumulator(id: 9, name: Some(internal.metrics.diskBytesSpilled), value: 0), internal.metrics.peakExecutionMemory -> LongAccumulator(id: 10, name: Some(internal.metrics.peakExecutionMemory), value: 0), internal.metrics.updatedBlockStatuses -> CollectionAccumulator(id: 11, name: Some(internal.metrics.updatedBlockStatuses), value: []), internal.metrics.shuffle.read.remoteBlocksFetched -> LongAccumulator(id: 12, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), internal.metrics.shuffle.read.localBlocksFetched -> LongAccumulator(id: 13, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 0), internal.metrics.shuffle.read.remoteBytesRead -> LongAccumulator(id: 14, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), internal.metrics.shuffle.read.remoteBytesReadToDisk -> LongAccumulator(id: 15, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0), internal.metrics.shuffle.read.localBytesRead -> LongAccumulator(id: 16, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 0), internal.metrics.shuffle.read.fetchWaitTime -> LongAccumulator(id: 17, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), internal.metrics.shuffle.read.recordsRead -> LongAccumulator(id: 18, name: Some(internal.metrics.shuffle.read.recordsRead), value: 0), internal.metrics.shuffle.write.bytesWritten -> LongAccumulator(id: 19, name: Some(internal.metrics.shuffle.write.bytesWritten), value: 0), internal.metrics.shuffle.write.recordsWritten -> LongAccumulator(id: 20, name: Some(internal.metrics.shuffle.write.recordsWritten), value: 0), internal.metrics.shuffle.write.writeTime -> LongAccumulator(id: 21, name: Some(internal.metrics.shuffle.write.writeTime), value: 0), internal.metrics.input.bytesRead -> LongAccumulator(id: 22, name: Some(internal.metrics.input.bytesRead), value: 0), internal.metrics.input.recordsRead -> LongAccumulator(id: 23, name: Some(internal.metrics.input.recordsRead), value: 0), internal.metrics.output.bytesWritten -> LongAccumulator(id: 24, name: Some(internal.metrics.output.bytesWritten), value: 0), internal.metrics.output.recordsWritten -> LongAccumulator(id: 25, name: Some(internal.metrics.output.recordsWritten), value: 0)), internalAccums -> null, _jvmGCTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _resultSerializationTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 5, _count -> 0), _executorRunTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 480, _count -> 0), shuffleWriteMetrics: org.apache.spark.executor.ShuffleWriteMetrics -> org.apache.spark.executor.ShuffleWriteMetrics(_bytesWritten -> LongAccumulator(id: 19, name: Some(internal.metrics.shuffle.write.bytesWritten), value: 0), _recordsWritten -> LongAccumulator(id: 20, name: Some(internal.metrics.shuffle.write.recordsWritten), value: 0), _writeTime -> LongAccumulator(id: 21, name: Some(internal.metrics.shuffle.write.writeTime), value: 0)), _memoryBytesSpilled: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), shuffleReadMetrics: org.apache.spark.executor.ShuffleReadMetrics -> org.apache.spark.executor.ShuffleReadMetrics(_localBytesRead -> LongAccumulator(id: 16, name: Some(internal.metrics.shuffle.read.localBytesRead), value: 0), _remoteBytesRead -> LongAccumulator(id: 14, name: Some(internal.metrics.shuffle.read.remoteBytesRead), value: 0), _fetchWaitTime -> LongAccumulator(id: 17, name: Some(internal.metrics.shuffle.read.fetchWaitTime), value: 0), _remoteBlocksFetched -> LongAccumulator(id: 12, name: Some(internal.metrics.shuffle.read.remoteBlocksFetched), value: 0), _recordsRead -> LongAccumulator(id: 18, name: Some(internal.metrics.shuffle.read.recordsRead), value: 0), _localBlocksFetched -> LongAccumulator(id: 13, name: Some(internal.metrics.shuffle.read.localBlocksFetched), value: 0), _remoteBytesReadToDisk -> LongAccumulator(id: 15, name: Some(internal.metrics.shuffle.read.remoteBytesReadToDisk), value: 0)), _diskBytesSpilled: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), org$apache$spark$executor$TaskMetrics$$_resultSize: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 264790, _count -> 0), _executorDeserializeTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 95, _count -> 0), externalAccums -> null, outputMetrics: org.apache.spark.executor.OutputMetrics -> org.apache.spark.executor.OutputMetrics(_bytesWritten -> LongAccumulator(id: 24, name: Some(internal.metrics.output.bytesWritten), value: 0), _recordsWritten -> LongAccumulator(id: 25, name: Some(internal.metrics.output.recordsWritten), value: 0)), inputMetrics: org.apache.spark.executor.InputMetrics -> org.apache.spark.executor.InputMetrics(_bytesRead -> LongAccumulator(id: 22, name: Some(internal.metrics.input.bytesRead), value: 0), _recordsRead -> LongAccumulator(id: 23, name: Some(internal.metrics.input.recordsRead), value: 0)), _updatedBlockStatuses: org.apache.spark.util.CollectionAccumulator -> org.apache.spark.util.CollectionAccumulator(_list -> []), _executorCpuTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 132684773, _count -> 0)), rddInfos -> List(RDD "MapPartitionsRDD" (5) StorageLevel: StorageLevel(1 replicas); CachedPartitions: 0; TotalPartitions: 4; MemorySize: 0.0 B; DiskSize: 0.0 B, RDD "MapPartitionsRDD" (1) StorageLevel: StorageLevel(1 replicas); CachedPartitions: 0; TotalPartitions: 4; MemorySize: 0.0 B; DiskSize: 0.0 B, RDD "MapPartitionsRDD" (2) StorageLevel: StorageLevel(1 replicas); CachedPartitions: 0; TotalPartitions: 4; MemorySize: 0.0 B; DiskSize: 0.0 B, RDD "ParallelCollectionRDD" (0) StorageLevel: StorageLevel(1 replicas); CachedPartitions: 0; TotalPartitions: 4; MemorySize: 0.0 B; DiskSize: 0.0 B, RDD "MapPartitionsRDD" (3) StorageLevel: StorageLevel(1 replicas); CachedPartitions: 0; TotalPartitions: 4; MemorySize: 0.0 B; DiskSize: 0.0 B, RDD "MapPartitionsRDD" (4) StorageLevel: StorageLevel(disk, memory, deserialized, 1 replicas); CachedPartitions: 0; TotalPartitions: 4; MemorySize: 0.0 B; DiskSize: 0.0 B)), properties: java.util.Properties -> java.util.Properties(serialVersionUID -> 4112578634029874840, defaults -> null, hexDigit: [C -> [C()))
          
          
          
          1535599573360: org.apache.spark.scheduler.SparkListenerBlockUpdated(blockUpdatedInfo: org.apache.spark.storage.BlockUpdatedInfo -> org.apache.spark.storage.BlockUpdatedInfo(diskSize -> 0, storageLevel: org.apache.spark.storage.StorageLevel -> org.apache.spark.storage.StorageLevel(org$apache$spark$storage$StorageLevel$$_useOffHeap -> false, org$apache$spark$storage$StorageLevel$$_useDisk -> false, org$apache$spark$storage$StorageLevel$$_deserialized -> false, org$apache$spark$storage$StorageLevel$$_useMemory -> true, org$apache$spark$storage$StorageLevel$$_replication -> 1), memSize -> 4654, blockManagerId: org.apache.spark.storage.BlockManagerId -> org.apache.spark.storage.BlockManagerId(org$apache$spark$storage$BlockManagerId$$executorId_ -> driver, org$apache$spark$storage$BlockManagerId$$host_ -> big16g, org$apache$spark$storage$BlockManagerId$$port_ -> 42081, org$apache$spark$storage$BlockManagerId$$topologyInfo_: scala.Option -> scala.None$(MODULE$ -> None, serialVersionUID -> 5066590221178148012)), blockId: org.apache.spark.storage.BlockId -> org.apache.spark.storage.BroadcastBlockId(broadcastId -> 0, field -> piece0)))
          
          1535599573406: org.apache.spark.scheduler.SparkListenerTaskStart(stageId -> 0, stageAttemptId -> 0, taskInfo: org.apache.spark.scheduler.TaskInfo -> org.apache.spark.scheduler.TaskInfo(killed -> false, host -> localhost, failed -> false, speculative -> false, executorId -> driver, launchTime -> 1535599573394, finishTime -> 1535599573583, attemptNumber -> 0, gettingResultTime -> 0, _accumulables -> List(AccumulableInfo(0,Some(number of output rows),Some(1),Some(1),true,true,Some(sql)), AccumulableInfo(5,Some(internal.metrics.resultSize),Some(66176),Some(66176),true,true,None), AccumulableInfo(4,Some(internal.metrics.executorCpuTime),Some(18976591),Some(18976591),true,true,None), AccumulableInfo(3,Some(internal.metrics.executorRunTime),Some(121),Some(121),true,true,None), AccumulableInfo(2,Some(internal.metrics.executorDeserializeCpuTime),Some(17395905),Some(17395905),true,true,None), AccumulableInfo(1,Some(internal.metrics.executorDeserializeTime),Some(26),Some(26),true,true,None)), taskLocality: scala.Enumeration$Value -> scala.Enumeration$Val(serialVersionUID -> -3501153230598116017, scala$Enumeration$Val$$i -> 0, name -> null), taskId -> 0, index -> 0))
                   
                   
          1535599573591: org.apache.spark.scheduler.SparkListenerTaskEnd(stageAttemptId -> 0, stageId -> 0, reason -> Success, taskType -> ResultTask, taskInfo: org.apache.spark.scheduler.TaskInfo -> org.apache.spark.scheduler.TaskInfo(killed -> false, host -> localhost, failed -> false, speculative -> false, executorId -> driver, launchTime -> 1535599573394, finishTime -> 1535599573583, attemptNumber -> 0, gettingResultTime -> 0, _accumulables -> List(AccumulableInfo(0,Some(number of output rows),Some(1),Some(1),true,true,Some(sql)), AccumulableInfo(5,Some(internal.metrics.resultSize),Some(66176),Some(66176),true,true,None), AccumulableInfo(4,Some(internal.metrics.executorCpuTime),Some(18976591),Some(18976591),true,true,None), AccumulableInfo(3,Some(internal.metrics.executorRunTime),Some(121),Some(121),true,true,None), AccumulableInfo(2,Some(internal.metrics.executorDeserializeCpuTime),Some(17395905),Some(17395905),true,true,None), AccumulableInfo(1,Some(internal.metrics.executorDeserializeTime),Some(26),Some(26),true,true,None)), taskLocality: scala.Enumeration$Value -> scala.Enumeration$Val(serialVersionUID -> -3501153230598116017, scala$Enumeration$Val$$i -> 0, name -> null), taskId -> 0, index -> 0), taskMetrics: org.apache.spark.executor.TaskMetrics -> org.apache.spark.executor.TaskMetrics(tempShuffleReadMetrics -> null, bitmap$trans$0 -> 10, testAccum: scala.Option -> scala.None$(MODULE$: scala.None$ -> scala.None$(MODULE$ -> None, serialVersionUID -> 5066590221178148012), serialVersionUID -> 5066590221178148012), _peakExecutionMemory: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _executorDeserializeCpuTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 17395905, _count -> 0), nameToAccums -> Map(internal.metrics.executorDeserializeTime -> LongAccumulator(id: 1, name: Some(internal.metrics.executorDeserializeTime), value: 26), internal.metrics.executorDeserializeCpuTime -> LongAccumulator(id: 2, name: Some(internal.metrics.executorDeserializeCpuTime), value: 17395905), internal.metrics.executorRunTime -> LongAccumulator(id: 3, name: Some(internal.metrics.executorRunTime), value: 121), internal.metrics.executorCpuTime -> LongAccumulator(id: 4, name: Some(internal.metrics.executorCpuTime), value: 18976591), internal.metrics.resultSize -> LongAccumulator(id: 5, name: Some(internal.metrics.resultSize), value: 66176), internal.metrics.jvmGCTime -> Un-registered Accumulator: LongAccumulator, internal.metrics.resultSerializationTime -> Un-registered Accumulator: LongAccumulator, internal.metrics.memoryBytesSpilled -> Un-registered Accumulator: LongAccumulator, internal.metrics.diskBytesSpilled -> Un-registered Accumulator: LongAccumulator, internal.metrics.peakExecutionMemory -> Un-registered Accumulator: LongAccumulator, internal.metrics.updatedBlockStatuses -> Un-registered Accumulator: CollectionAccumulator, internal.metrics.shuffle.read.remoteBlocksFetched -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.read.localBlocksFetched -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.read.remoteBytesRead -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.read.remoteBytesReadToDisk -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.read.localBytesRead -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.read.fetchWaitTime -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.read.recordsRead -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.write.bytesWritten -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.write.recordsWritten -> Un-registered Accumulator: LongAccumulator, internal.metrics.shuffle.write.writeTime -> Un-registered Accumulator: LongAccumulator, internal.metrics.input.bytesRead -> Un-registered Accumulator: LongAccumulator, internal.metrics.input.recordsRead -> Un-registered Accumulator: LongAccumulator, internal.metrics.output.bytesWritten -> Un-registered Accumulator: LongAccumulator, internal.metrics.output.recordsWritten -> Un-registered Accumulator: LongAccumulator), internalAccums -> null, _jvmGCTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _resultSerializationTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _executorRunTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 121, _count -> 0), externalAccums: scala.collection.mutable.ArrayBuffer -> scala.collection.mutable.ArrayBuffer(serialVersionUID -> 1529165946227428979, initialSize -> 16, array: [Ljava.lang.Object; -> [Ljava.lang.Object;(), size0 -> 1), shuffleWriteMetrics: org.apache.spark.executor.ShuffleWriteMetrics -> org.apache.spark.executor.ShuffleWriteMetrics(_bytesWritten: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _recordsWritten: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _writeTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0)), _memoryBytesSpilled: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), shuffleReadMetrics: org.apache.spark.executor.ShuffleReadMetrics -> org.apache.spark.executor.ShuffleReadMetrics(_localBytesRead: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _fetchWaitTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _remoteBytesRead: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _localBlocksFetched: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _remoteBytesReadToDisk: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _recordsRead: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _remoteBlocksFetched: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0)), _diskBytesSpilled: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), org$apache$spark$executor$TaskMetrics$$_resultSize: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 66176, _count -> 0), _executorDeserializeTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 26, _count -> 0), outputMetrics: org.apache.spark.executor.OutputMetrics -> org.apache.spark.executor.OutputMetrics(_bytesWritten: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _recordsWritten: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0)), inputMetrics: org.apache.spark.executor.InputMetrics -> org.apache.spark.executor.InputMetrics(_bytesRead: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0), _recordsRead: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 0, _count -> 0)), _updatedBlockStatuses: org.apache.spark.util.CollectionAccumulator -> org.apache.spark.util.CollectionAccumulator(_list -> []), _executorCpuTime: org.apache.spark.util.LongAccumulator -> org.apache.spark.util.LongAccumulator(_sum -> 18976591, _count -> 0)))
          
          1535599575437: org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart(physicalPlanDescription -> == Parsed Logical Plan ==
          InsertIntoHadoopFsRelationCommand file:/tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data, false, Parquet, Map(path -> /tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data), ErrorIfExists, [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79]
          +- AnalysisBarrier
                +- Repartition 1, true
          	 +- LocalRelation [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79]
          
          == Analyzed Logical Plan ==
          InsertIntoHadoopFsRelationCommand file:/tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data, false, Parquet, Map(path -> /tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data), ErrorIfExists, [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79]
          +- Repartition 1, true
             +- LocalRelation [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79]
          
          == Optimized Logical Plan ==
          InsertIntoHadoopFsRelationCommand file:/tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data, false, Parquet, Map(path -> /tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data), ErrorIfExists, [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79]
          +- Repartition 1, true
             +- LocalRelation [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79]
          
          == Physical Plan ==
          Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data, false, Parquet, Map(path -> /tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data), ErrorIfExists, [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79]
          +- Exchange RoundRobinPartitioning(1)
             +- LocalTableScan [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79], description -> parquet at LogisticRegression.scala:1233, sparkPlanInfo: org.apache.spark.sql.execution.SparkPlanInfo -> org.apache.spark.sql.execution.SparkPlanInfo(nodeName -> Execute InsertIntoHadoopFsRelationCommand, simpleString -> Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data, false, Parquet, Map(path -> /tmp/spark-logistic-regression-model/stages/2_logreg_4e386bed3555/data), ErrorIfExists, [numClasses#75, numFeatures#76, interceptVector#77, coefficientMatrix#78, isMultinomial#79], children -> List(org.apache.spark.sql.execution.SparkPlanInfo@ab6dcc6f), metrics -> ArrayBuffer(org.apache.spark.sql.execution.metric.SQLMetricInfo@6ebe00f9, org.apache.spark.sql.execution.metric.SQLMetricInfo@7fb662ad, org.apache.spark.sql.execution.metric.SQLMetricInfo@653502e5, org.apache.spark.sql.execution.metric.SQLMetricInfo@57846715)), details -> org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547) ... call-stack-details-omitted-in-this-sample-extract ... sbt.Run.$anonfun$run$4(Run.scala:77), time -> 1535599575436, executionId -> 0)
          
          1535599577433: org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd(executionId -> 2, time -> 1535599577432)
          1535599577436: org.apache.spark.scheduler.SparkListenerApplicationEnd(time -> 1535599577436)
